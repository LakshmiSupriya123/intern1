{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae823e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\NAVEEN\n",
      "[nltk_data]     REDDY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x00'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m maxlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     21\u001b[0m pickle_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpickle_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m pickle_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m word_vector \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(pickle_in)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '\\x00'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from htbuilder import HtmlElement, div, ul, li, br, hr, a, p, img, styles, classes, fonts\n",
    "from htbuilder.units import percent, px\n",
    "from htbuilder.funcs import rgba, rgb\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = 'model.pkl'\n",
    "maxlen = 100\n",
    "pickle_in = open(model, 'rb')\n",
    "model = pickle.load(pickle_in)\n",
    "pickle_in = open(\"vectorizer.pkl\", 'rb')\n",
    "word_vector = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
    "    in lowercase'''\n",
    "    \n",
    "    sentence = sen.lower()\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "    # Remove Stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "\n",
    "    # create sentence embeddings\n",
    "    wordvec = word_tokenizer.texts_to_sequences([sentence])\n",
    "    wordvec = pad_sequences(wordvec, padding='post', maxlen=maxlen)\n",
    "\n",
    "    return wordvec\n",
    "\n",
    "def checkNews(txt):\n",
    "    if(txt!=''):\n",
    "        wordvec = preprocess_text(txt)\n",
    "        rating = loaded_model.predict(wordvec)\n",
    "        prediction_text= f\"Probable IMDb rating is: {np.round(rating[0][0]*10,1)}\"\n",
    "        st.write(prediction_text)\n",
    "\n",
    "st.title('Sentiment Analysis')\n",
    "\n",
    "news = st.text_area('Enter review:')\n",
    "checkNews(news)\n",
    "\n",
    "from htbuilder import HtmlElement, div, ul, li, br, hr, a, p, img, styles, classes, fonts\n",
    "from htbuilder.units import percent, px\n",
    "from htbuilder.funcs import rgba, rgb\n",
    "\n",
    "\n",
    "def image(src_as_string, **style):\n",
    "    return img(src=src_as_string, style=styles(**style))\n",
    "\n",
    "\n",
    "def link(link, text, **style):\n",
    "    return a(_href=link, _target=\"_blank\", style=styles(**style))(text)\n",
    "\n",
    "\n",
    "def layout(*args):\n",
    "\n",
    "    style = \"\"\"\n",
    "    <style>\n",
    "      # MainMenu {visibility: hidden;}\n",
    "      footer {visibility: hidden;}\n",
    "     .stApp { bottom: 105px; }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    style_div = styles(\n",
    "        position=\"fixed\",\n",
    "        left=0,\n",
    "        bottom=0,\n",
    "        margin=px(0, 0, 0, 0),\n",
    "        width=percent(100),\n",
    "        color=\"#F63366\",\n",
    "        text_align=\"center\",\n",
    "        height=\"auto\",\n",
    "        opacity=1\n",
    "    )\n",
    "\n",
    "    style_hr = styles(\n",
    "        display=\"block\",\n",
    "        margin=px(8, 8, \"auto\", \"auto\"),\n",
    "        border_width=px(1)\n",
    "    )\n",
    "\n",
    "    body = p()\n",
    "    foot = div(\n",
    "        style=style_div\n",
    "    )(\n",
    "        hr(\n",
    "            style=style_hr\n",
    "        ),\n",
    "        body\n",
    "    )\n",
    "\n",
    "    st.markdown(style, unsafe_allow_html=True)\n",
    "\n",
    "    for arg in args:\n",
    "        if isinstance(arg, str):\n",
    "            body(arg)\n",
    "\n",
    "        elif isinstance(arg, HtmlElement):\n",
    "            body(arg)\n",
    "\n",
    "    st.markdown(str(foot), unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# def footer():\n",
    "#     myargs = [\n",
    "#         \"Made in \",\n",
    "#         image('https://assets.website-files.com/5dc3b47ddc6c0c2a1af74ad0/5e181828ba9f9e92b6ebc6e7_RGB_Logomark_Color_Light_Bg.png',\n",
    "#               width=px(25), height=px(25)),\n",
    "#         \" by imt-02\",\n",
    "#         br(),\n",
    "#         link(\"https://github.com/alooperalta/Fake-News-Detection-System\", image('https://github.com/alooperalta/Fake-News-Detection-System/blob/main/gitLogo.png?raw=true',height=\"40px\")),\n",
    "#     ]\n",
    "#     layout(*myargs)\n",
    "\n",
    "def footer():\n",
    "    myargs = [\n",
    "        \"Made in \",\n",
    "        image('https://assets.website-files.com/5dc3b47ddc6c0c2a1af74ad0/5e181828ba9f9e92b6ebc6e7_RGB_Logomark_Color_Light_Bg.png',\n",
    "              width=px(25), height=px(25)),\n",
    "        \" by Anurag Parashar & Utkarsh Gupta\",\n",
    "        br(),\n",
    "        link(\"https://github.com/AnuragParashar2000/Movie-Review-Sentiment-Analysis\", image('https://github.com/alooperalta/Fake-News-Detection-System/blob/main/gitLogo.png?raw=true',height=\"40px\")),\n",
    "    ]\n",
    "    layout(*myargs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    footer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae75e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
